{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预备知识\n",
    "\n",
    "### tf.where(条件语句，A(真)，B(假))\n",
    "\n",
    "    a = tf.constant([1,2,3,1,1])\n",
    "    b = tf.constant([0,1,3,4,5])\n",
    "\n",
    "### tf.greater()比较对应元素 (返回True或False的列表)\n",
    "\n",
    "    c = tf.where(tf.greater(a,b),a,b)\n",
    "    # 若a>b，返回a<b>对应位置</b>的元素，否则返回b<b>对应位置</b>的元素\n",
    "    【out:】 tf.Tensor([1 2 3 4 5],shape=(5),dtype=int32)\n",
    "    \n",
    "### tf.random.RandomState.rand(维度)\n",
    "\n",
    "    返回一个[0,1)之间的随机数（ 维度为空、返回张量）\n",
    "    rdm = np.random.RandomState(seed=1)#seed常数每次生成的随机数相同\n",
    "\n",
    "    a = rdm.rand() #返回一个随机标量\n",
    "    b = rdm.rand(2,3)#返回一个2行3列的随机数矩阵\n",
    "    print(\"a\",a)\n",
    "    print(\"b\",b)\n",
    "    【out:】\n",
    "    a:0.417022...........\n",
    "    b:[[7.20  .. ...],[.. .. ..]]\n",
    "\n",
    "### np.vstack()\n",
    "\n",
    "    将两个数组按<b>垂直方向</b>叠加\n",
    "\n",
    "    a = np.array([1,2,3])\n",
    "    b = np.array([4,5,6])\n",
    "    c = np.vstack((a,b))\n",
    "    print(\"c:\",c)\n",
    "    【out:】\n",
    "    c:\n",
    "    [[1 2 3],\n",
    "    [4 5 6]]\n",
    "\n",
    "\n",
    "### np.mgrid[]  & np.ravel  & np.c_[] 生成网格坐标点\n",
    "    \n",
    "    # 返回若干组维度相同的等差数组 [)\n",
    "    np.mgrid[起始值：结束值：步长，起始值：结束值：步长,.....] \n",
    "    \n",
    "    \n",
    "    #把x变成一维数组\n",
    "    np.ravel()\n",
    "    \n",
    "    # 使返回的间隔数值点配对输出\n",
    "    np.c_[数组1，数组2，...]    \n",
    "\n",
    "    x,y = np.mgrid[1:3:1,2:4:0.5]\n",
    "    grid = np.c_[x.ravel(),y.ravel()]\n",
    "    print('x',x)\n",
    "    print('y',y)\n",
    "    print('grid:\\n',grid)\n",
    "    【out:】\n",
    "    x = [[1. 1. 1. 1.]\n",
    "        [2. 2. 2. 2.]]\n",
    "    y = [[2. 2.5 3. 3.5]\n",
    "        [2. 2.5 3. 3.5]]\n",
    "    grid:（拉直配对）\n",
    "    [[1.2.]\n",
    "    [1. 2.5]\n",
    "    [1.3.]\n",
    "    [1.3.5]\n",
    "    [2.2.]\n",
    "    [2.2.5]\n",
    "    [2. 3.]\n",
    "    [2.3.5]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "# 常见函数\n",
    "\n",
    "## 指数衰减学习率\n",
    "\n",
    "指数衰减学习率 = 初始学习率 * 学习率衰减率 ** （当前轮数 / 多少轮衰减一次）\n",
    "\n",
    "## sigmoid函数  \n",
    "\n",
    "特点：\n",
    "+ 映射到 0~1\n",
    "+ 常用于选择工具\n",
    "+ 易造成梯度消失\n",
    "+ 输出非0均值，收敛慢\n",
    "+ 幂运算复杂，训练时间长\n",
    "\n",
    "## tanh函数 \n",
    "\n",
    "+ 0均值 \n",
    "+ 常用于转换工具\n",
    "\n",
    "## Relu \n",
    "\n",
    "+ 最常用的激活函数\n",
    "+ 收敛速度远大于上面两者\n",
    "\n",
    "## 对初学者的建议：\n",
    "+ 首选Relu激活函数\n",
    "+ 学习率设置较小值\n",
    "+ 输入特征标准化： 即让输入特征满足以0为均值，1为标准差的正态分布\n",
    "+ 初始参数中心化：即让随机生成的参数满足以0为均值，$\\sqrt{{2}\\over{当前层输入特征个数}}$为标准差的正态分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "# 损失函数\n",
    "\n",
    "##### 预测值y与已知答案y_之间的差距\n",
    "\n",
    "## 1.均方误差mse （Mean Squared Errot）\n",
    "$${MSE(y_,y) }={ \\sum{(y-y\\_ )}^2 \\over n}$$\n",
    "\n",
    "    loss_mse = tf.reduce_mean(tf.square(y_ - y))\n",
    "    \n",
    "当使用均方误差mse，默认认为预测多了和预测少了损失是一样的，但是实际上，预测多了损失的可能是成本，预测少了损失的可能是利润。利润和成本往往不相等（参见2.自定义）\n",
    "\n",
    "## 2.自定义\n",
    "\n",
    "如预测酸奶销量、酸奶成本COST1元，利润PROFIT99元。预测少了损失利润99元，预测少了损失成本1元。所以希望生成的预测函数往多了预测\n",
    "\n",
    "    loss_zdy = tf.reduce_sum(tf where :((tf.greater(y,y_),cost(y-y_),profit(y_-y)))\n",
    "\n",
    "## 3.交叉熵ce (Cross Entropy)\n",
    "\n",
    "交叉熵表示两个概率分布之间的距离，**交叉熵越大，两个概率分布越远。**\n",
    "\n",
    "##### eg.二分类：\n",
    "\n",
    "已知答案y_ = (1,0)表示第一种情况发生的概率为1，第二种情况发生的概率0，\n",
    "预测y1 = (0.6,0.4) y2 = (0.8,0.2)哪个更接近标准答案y_?\n",
    "\n",
    "    答：H1((1,0),(0.6,0.4)) = 0.511\n",
    "    H2(...,...) = 0.223\n",
    "    H1 > H2\n",
    "    所以y2预测更准\n",
    "    \n",
    "    loss_ce1 = tf.losses.categorical_crossentropy([1,0],[0.6,0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax与交叉熵结合\n",
    "\n",
    "输出先经过softmax函数，再计算y与y_的交叉熵损失函数\n",
    "\n",
    "    tf.nn.softmax_cross_entropy_with_logits(y_,y)`\n",
    "    # y_是正确答案\n",
    "    y_ = np..array([[1 0 0],[0 1 0],[0 0 1],[1 0 0],[0 1 0]])\n",
    "     \n",
    "    y = np.array([[12, 3, 2], [3, 10, 1], [1, 2, 5], [4, 6.5, 1.2], [3, 6, 1]])     \n",
    "    y_pro = tf.nn.softnax(y)\n",
    "    \n",
    "    # 分布计算\n",
    "    loss_ce1 = tf.losses.categorical_crossentropy(y_,y_pro)\n",
    "    # 等价于综合计算\n",
    "    loss_ce2 = tf.nn.softmax_cross_entropy_with_logits(y_,y)\n",
    "    # 计算结果loss_ce1 = loss_ce2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数优化器\n",
    "\n",
    "待优化参数w、损失函数loss、学习率lr、每迭代一个batch，t表示当前batch迭代的总次数\n",
    "1. 计算t时刻损失函数关于当前参数的梯度${g_t} = {Δloss} $=   ${əloss} \\over əw_t$\n",
    "2. 计算t时刻一阶动量$m_t$和二阶动量$V_t$\n",
    "    + 一阶动量：与梯度相关的函数\n",
    "    + 二阶动量：与梯度平方相关的函数\n",
    "3. 计算t时刻下降梯度：$ŋ_t = lr * m_t / \\sqrt{V_t}$(学习率乘以一阶动量除以根号下二阶动量)\n",
    "\n",
    "4. 计算t+1时刻参数：$W_{t+1} = W_t - ŋ_t = W_t - lr * m_t / \\sqrt{V_t}$（当前时刻的参数减去学习率✖）\n",
    "\n",
    "## SGD 无momentum，常用的梯度下降\n",
    "$W_{t+1} = $ $W_t - lr * $ $əloss \\over əw_t^*$\n",
    "\n",
    "    w1.assign_sub(lr * grads[0])\n",
    "    b1.assign_sub(lr * grads[1])\n",
    "\n",
    "## SGDM 含有momentum的SGD，在SGD基础增加一阶动量\n",
    "$m_t = β * m_{t-1} + (1 - β) * g_t$\n",
    "\n",
    "而且上一时刻的一阶动量$m_{t-1}$占大头，因为β经验值接近0.9\n",
    "二阶动量在SGDM中仍是恒等于1的 $V_t = 1$\n",
    "\n",
    "把一阶动量和二阶动量带入ŋ的计算公式，\n",
    "\n",
    "$ŋ_t = lr * m_t / \\sqrt{V_t}$ = $lr * m_t$ = $ lr * (β * m_{t-1} + (1-β) * g_t)$\n",
    "\n",
    "再把ŋ带入参数更新公式就可以用SGDM更新参数了\n",
    "\n",
    "$W_{t+1} = W_t - ŋ_t$ = $ W_t - lr * (β * m_{t-1} + (1 - β) * g_t)$\n",
    "\n",
    "用python实现的最重要的一步是把一阶动量和二阶动量计算出来\n",
    "\n",
    "$m_t = β * m_{t-1} + (1 - β) * g_t$\n",
    "\n",
    "$g_t$是当前时刻的梯度\n",
    "\n",
    "    m_w, m_b = 0,0 # 第0时刻的一阶动量\n",
    "    beta = 0.9\n",
    "    \n",
    "    # sgd-momentum\n",
    "    m_w = beta * m_w + (1 - beta) * grads[0]\n",
    "    m_b = beta * m_b + (1 - beta) * grads[1]\n",
    "    w1.assign_sub(lr * m_w)\n",
    "    b1.assign_sub(lr * m_b)\n",
    "\n",
    "\n",
    "# Adagrad，在SGD的基础上增加二阶动量\n",
    "`可以对模型中的每个参数分配自适应学习率了`\n",
    "\n",
    "adagrad的一阶动量和SGD一样，是当前的梯度\n",
    "$m_t = g_t$\n",
    "\n",
    "二阶动量是从开始到现在、梯度平方的累加和\n",
    "$V_t = \\sum^{t}_{t=1} g_t^2$\n",
    "\n",
    "有了一阶动量和二阶动量，可以带入参数更新参数了\n",
    "$ŋ_t = lr * m_t / (\\sqrt{V_t})$  = $lr * g_t / (\\sqrt{\\sum g_t^2})$\n",
    "\n",
    "$W_{t+1} = W_t - ŋ_t = W_t - lr * g_t / (\\sqrt{\\sum g_t^2})$\n",
    "\n",
    "adagrad的一阶动量mt就是当前时刻的梯度\n",
    "$m_t = g_t  V_t = \\sum^{t}_{t=1} g_t^2$\n",
    "\n",
    "所以直接带入参数更新公式\n",
    "设0时刻 w和b的二阶动量初始值是0\n",
    "\n",
    "    v_w,v_b = 0,0\n",
    "    # adagrad\n",
    "    v_w += tf.square(grads[0])# 梯度平方累计和\n",
    "    v_b += tf.square(grads[1])\n",
    "    w1.assign_sub(lr * grads[0] / tf.sqrt(v_w))\n",
    "    b1_assign_sub(lr * grads[1] / tf.sqrt(v_b))\n",
    "\n",
    "\n",
    "# RMSProp是在SGD基础上增加二阶动量\n",
    "\n",
    "二阶动量v使用煮熟滑动平均值计算，表征的是过去一段时间的平均值\n",
    "\n",
    "同样，求出一阶动量和二阶动量后\n",
    "\n",
    "$m_t = g_t$\n",
    "$V_t = β * V_{t-1} + (1 - β) * g^2_t$\n",
    "\n",
    "代入参数更新公式\n",
    "$ŋ_t = lr * m_t / (\\sqrt{V_t})$  = $ lr * g_t / (\\sqrt{β * V_{t-1} + (1 - β) * g_t^2} )$\n",
    "\n",
    "$W_{t+1} = W_t - ŋ_t$ = $w_t - lr * g_t /(\\sqrt{β * V_{t-1} + (1 - β) * g_t^2} ) $\n",
    "\n",
    "实现参数自更新\n",
    "\n",
    "一阶动量$m_t$是梯度\n",
    "\n",
    "$M_t = g_t V_t = β * V_{t-1} + (1 - β) * g^2_t$\n",
    "    \n",
    "    v_w,v_b = 0,0\n",
    "    beta = 0.9\n",
    "    v_w = beta * v_w + (1 - beta) * tf.square(grads[0])\n",
    "    v_b = beta * v_b + (1 - beta) * tf.square(grads[1])\n",
    "    w1.assign_sub(lr * grads[0] / tf.sqrt(v_w)\n",
    "    b1.assign_sub(lr * grads[1] / tf.sqrt(v_b)\n",
    "\n",
    "# adam \n",
    "\n",
    "同时结合SGDM一阶动量和RMSProp二阶动量，并在此基础上增加了`两个修正项`\n",
    "$m_t = β_1 * m_{t-1} + (1 - β_1) * g_t$\n",
    "\n",
    "修正一阶动量的偏差：$m_t $=$ m_t \\over {1 - β_1^t}$\n",
    "\n",
    "$V_t = β_2 * V_{step-1} + (1 - β_2) * g_t^2$\n",
    "\n",
    "修正二阶动量的偏差：$V_t = V_t \\over (1 - β_2^t)$\n",
    "\n",
    "$ŋ_t = lr * m_t / \\sqrt{V_t }$ \n",
    "\n",
    "$W_{t+1} = W_t - ŋ_t$\n",
    "\n",
    "    m_w，m_b = 0,0\n",
    "    v_w,v_b = 0,0\n",
    "    beta1, beta2 = 0.9,0.999\n",
    "    delta_w, delta_b = 0,0\n",
    "    global_step = 0\n",
    "    # global_step 是训练开始到当前时刻所经历的总batch数\n",
    "    \n",
    "    #adam一阶动量表达式和含momentum的sgd一阶动量表达式是相同的\n",
    "    m_w = beta1 * m_w + (1 - beta1) * grads[0]\n",
    "    m_b = beta1 * m_b + (1 - beta1) * grads[1]    \n",
    "    # 二阶动量表达式和RMSProp的二阶动量表达式是一样的\n",
    "    v_w = beta2 * v_w + (1 - beta2) * tf.square(grads[0])\n",
    "    v_b = beta2 * v_b + (1 - beta2) * tf.square(grads[1])\n",
    "    \n",
    "    m_w_correction = m_w / (1 - tf.pow(beta1,int(global_step)))\n",
    "    m_b_correction = m_b / (1 - tf.pow(beta1,int(global_step)))\n",
    "    v_w_correction = v_w / (1 - tf.pow(beta2,int(global_step)))\n",
    "    v_b_correction = v_b / (1 - tf.pow(beta2,int(global_step)))\n",
    "    \n",
    "    # 把修正项带入参数更新公式\n",
    "    w1.assign_sub(lr * m_w_correction / tf.sqrt(v_w_correction))\n",
    "    b1.assign_sub(lr * m_b_correction / tf.sqrt(v_b_correction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
