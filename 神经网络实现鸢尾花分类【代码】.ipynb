{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [方法介绍](https://www.bilibili.com/video/BV1AK411L77v?p=7)\n",
    "    \n",
    "#### 1.使用同样的随机数种子，使输入特征/标签一一对应\n",
    "    np.random.seed(116)    \n",
    "    np.random.shuffle(x_data)\n",
    "    np.random.seed(116)\n",
    "    np.random.shuffle(y_data)\n",
    "\n",
    "#### 2.数据集分出永不相见的训练集和测试集\n",
    "    x_train = x_data[:-30]\n",
    "    y_train = y_data[:-30]\n",
    "    x_test = x_data[-30:]\n",
    "    y_test = y_data[-30:]\n",
    "\n",
    "\n",
    "#### 3.配成【输入特征，标签】对，每次喂入一小撮(batch)、每32组打包成一个batch\n",
    "    train_db = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(32)\n",
    "    test_db = tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(32)\n",
    "\n",
    "#### 4.定义神经网络中所有可训练参数\n",
    "    w1 = tf.Variable(tf.random.truncated_normal([4,3],stddev=0.1,seed=1))\n",
    "    b1 = tf.Variable(tf.random.truncated_normal([3],stddev=0.1,seed=1)\n",
    "> 因为输入特征是4个，所以是4.又只用一层网络，输出节点等于分类数等于3。w1就是4行3列的张量\n",
    "b1必须要与w1的3一致\n",
    "\n",
    "#### 5.计算准确率\n",
    "    for epoch in range(epoch):# 数据集级别迭代        \n",
    "        for step,(x_train,y_train) in enumerate(train_db):# batch级别迭代\n",
    "            with tf.GradientTape() as tape: #记录梯度信息\n",
    "                前向传播计算y\n",
    "                计算总loss\n",
    "             grads = tape.gradient(loss,[w1,b1]) #损失函数loss对w1,b1计算偏导数\n",
    "             w1.assign_sub(lr*grads[0])#参数自更新\n",
    "             b1.assign_sub(lr*grads[1])\n",
    "        print(\"Epoch{},loss{}\".format(epoch,loss_all/4))#打印出本轮的数值\n",
    "> 训练集有120组数据,tatch是32个，每个step只能喂入32组数据，需要batch级别循环4次，除以4求得每次step迭代的平均值loss 120/32 = 3.75\n",
    "\n",
    "\n",
    "    希望每个epoch循环后可以显示当前模型的效果，所以在每个epoch循环嵌套了一个batch级别的循环\n",
    "    for x_test,y_test in test_db:\n",
    "        y = tf.matmul(h,w) + b #前向传播计算出y为预测结果 matmul矩阵相乘\n",
    "        y = tf.nn.softmax(y) # 使其y符合概率分布\n",
    "        pred = tf.argmax(y,axis=1) #返回y中最大值的索引，即预测对的分类\n",
    "        pred = tf.cast(pred,dtype=y_test.dtype)#  调整数据类型与标签一致\n",
    "        correct = tf.cast(tf.equal(pred,y_test),dtype=tf.int32) #如果预测值与标签一致，correct子架1\n",
    "        correct = tf.reduce_sum(correct) #把每个batch的correct数加起来\n",
    "        total_correct += int (correct) #将所有batch中的correct数加起来\n",
    "        total_num += x_test.shape[0]\n",
    "     acc = total_correct / total_num\n",
    "     print(\"test_acc:\\n\",acc)\n",
    "\n",
    "\n",
    "####  6.可视化\n",
    "         plt.title('Acc Curve')\n",
    "         plt.xlabel('Epoch')\n",
    "         plt.ylabel('Acc')\n",
    "         plt.plot(test_acc,lable=\"$Accuracy$\") #逐点画出test_acc并连线\n",
    "         plt.legend()\n",
    "         plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.2821310982108116\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 1, loss: 0.25459615513682365\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 2, loss: 0.22570249810814857\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 3, loss: 0.21028399839997292\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 4, loss: 0.19942265003919601\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 5, loss: 0.18873637914657593\n",
      "Test_acc: 0.5\n",
      "--------------------------\n",
      "Epoch 6, loss: 0.17851299047470093\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 7, loss: 0.16922875493764877\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 8, loss: 0.16107673197984695\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 9, loss: 0.15404684841632843\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 10, loss: 0.14802725985646248\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 11, loss: 0.14287303015589714\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 12, loss: 0.1384414155036211\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 13, loss: 0.13460607640445232\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 14, loss: 0.13126072846353054\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 15, loss: 0.12831822223961353\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 16, loss: 0.12570795230567455\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 17, loss: 0.12337299063801765\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 18, loss: 0.12126746587455273\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 19, loss: 0.11935433372855186\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 20, loss: 0.11760355532169342\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 21, loss: 0.11599067971110344\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 22, loss: 0.11449568346142769\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 23, loss: 0.11310207843780518\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 24, loss: 0.11179621890187263\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 25, loss: 0.11056671477854252\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 26, loss: 0.10940408334136009\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 27, loss: 0.10830028541386127\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 28, loss: 0.10724855214357376\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 29, loss: 0.10624313168227673\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 30, loss: 0.1052791029214859\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 31, loss: 0.10435221716761589\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 32, loss: 0.10345886647701263\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 33, loss: 0.10259587876498699\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 34, loss: 0.10176052898168564\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 35, loss: 0.10095042176544666\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 36, loss: 0.10016347281634808\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 37, loss: 0.09939785301685333\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 38, loss: 0.0986519306898117\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 39, loss: 0.09792429022490978\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 40, loss: 0.09721364825963974\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 41, loss: 0.09651889838278294\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 42, loss: 0.09583901055157185\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 43, loss: 0.09517310559749603\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 44, loss: 0.09452036581933498\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 45, loss: 0.0938800759613514\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 46, loss: 0.09325156174600124\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 47, loss: 0.09263424947857857\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 48, loss: 0.09202760085463524\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 49, loss: 0.09143111668527126\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 50, loss: 0.09084436297416687\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 51, loss: 0.09026693925261497\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 52, loss: 0.08969846926629543\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 53, loss: 0.08913860656321049\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 54, loss: 0.08858705498278141\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 55, loss: 0.08804351650178432\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 56, loss: 0.08750772476196289\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 57, loss: 0.0869794450700283\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 58, loss: 0.08645843528211117\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 59, loss: 0.08594449050724506\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 60, loss: 0.08543741703033447\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 61, loss: 0.08493701927363873\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 62, loss: 0.08444313891232014\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 63, loss: 0.08395560272037983\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 64, loss: 0.08347426354885101\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 65, loss: 0.08299898356199265\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 66, loss: 0.08252961561083794\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 67, loss: 0.08206603862345219\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 68, loss: 0.08160812221467495\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 69, loss: 0.08115577697753906\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 70, loss: 0.08070887625217438\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 71, loss: 0.08026730827987194\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 72, loss: 0.07983098551630974\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 73, loss: 0.07939981296658516\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 74, loss: 0.0789736919105053\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 75, loss: 0.07855254225432873\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 76, loss: 0.07813626900315285\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 77, loss: 0.07772481068968773\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 78, loss: 0.07731806673109531\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 79, loss: 0.07691597566008568\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 80, loss: 0.07651844993233681\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 81, loss: 0.07612543925642967\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 82, loss: 0.0757368616759777\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 83, loss: 0.07535265013575554\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 84, loss: 0.07497274875640869\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 85, loss: 0.07459708396345377\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 86, loss: 0.074225596152246\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 87, loss: 0.07385822664946318\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 88, loss: 0.07349492143839598\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 89, loss: 0.07313562091439962\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 90, loss: 0.07278026547282934\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 91, loss: 0.0724287973716855\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, loss: 0.07208118494600058\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 93, loss: 0.07173734344542027\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 94, loss: 0.07139724027365446\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 95, loss: 0.07106082234531641\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 96, loss: 0.07072803378105164\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 97, loss: 0.0703988391906023\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 98, loss: 0.07007317896932364\n",
      "Test_acc: 0.8333333333333334\n",
      "--------------------------\n",
      "Epoch 99, loss: 0.0697510140016675\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 100, loss: 0.06943229213356972\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 101, loss: 0.06911696959286928\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 102, loss: 0.06880500447005033\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 103, loss: 0.068496348336339\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 104, loss: 0.06819095648825169\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 105, loss: 0.06788879167288542\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 106, loss: 0.06758981943130493\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 107, loss: 0.0672939820215106\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 108, loss: 0.06700124125927687\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 109, loss: 0.0667115617543459\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 110, loss: 0.06642490904778242\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 111, loss: 0.06614123564213514\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 112, loss: 0.0658605070784688\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 113, loss: 0.06558268051594496\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 114, loss: 0.06530772615224123\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 115, loss: 0.06503560487180948\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 116, loss: 0.06476627103984356\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 117, loss: 0.06449970323592424\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 118, loss: 0.06423585396260023\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 119, loss: 0.06397469434887171\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 120, loss: 0.06371618993580341\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 121, loss: 0.06346031092107296\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 122, loss: 0.06320700887590647\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 123, loss: 0.06295627448707819\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 124, loss: 0.0627080425620079\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 125, loss: 0.06246231310069561\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 126, loss: 0.06221904046833515\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 127, loss: 0.061978183686733246\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 128, loss: 0.06173973251134157\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 129, loss: 0.06150364316999912\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 130, loss: 0.061269884929060936\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 131, loss: 0.06103843171149492\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 132, loss: 0.06080925837159157\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 133, loss: 0.06058232951909304\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 134, loss: 0.06035762187093496\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 135, loss: 0.060135108418762684\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 136, loss: 0.05991474911570549\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 137, loss: 0.05969652533531189\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 138, loss: 0.05948041286319494\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 139, loss: 0.05926638375967741\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 140, loss: 0.05905440915375948\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 141, loss: 0.058844463899731636\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 142, loss: 0.05863652843981981\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 143, loss: 0.058430567383766174\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 144, loss: 0.05822655465453863\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 145, loss: 0.05802448093891144\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 146, loss: 0.057824309915304184\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 147, loss: 0.057626024819910526\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 148, loss: 0.0574295949190855\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 149, loss: 0.0572349950671196\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 150, loss: 0.05704221222549677\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 151, loss: 0.05685121566057205\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 152, loss: 0.05666199512779713\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 153, loss: 0.05647451803088188\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 154, loss: 0.056288767606019974\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 155, loss: 0.05610471125692129\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 156, loss: 0.055922345258295536\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 157, loss: 0.0557416332885623\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 158, loss: 0.05556256230920553\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 159, loss: 0.05538512021303177\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 160, loss: 0.05520927347242832\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 161, loss: 0.0550350034609437\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 162, loss: 0.05486230552196503\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 163, loss: 0.05469114240258932\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 164, loss: 0.05452151130884886\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 165, loss: 0.05435337871313095\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 166, loss: 0.054186731576919556\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 167, loss: 0.054021554067730904\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 168, loss: 0.0538578312844038\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 169, loss: 0.053695546463131905\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 170, loss: 0.0535346744582057\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 171, loss: 0.05337520595639944\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 172, loss: 0.05321711394935846\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 173, loss: 0.05306038819253445\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 174, loss: 0.05290501844137907\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 175, loss: 0.05275098513811827\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 176, loss: 0.0525982566177845\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 177, loss: 0.052446840330958366\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 178, loss: 0.05229670833796263\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 179, loss: 0.05214785039424896\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 180, loss: 0.052000245079398155\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 181, loss: 0.05185388680547476\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 182, loss: 0.05170875135809183\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183, loss: 0.05156483128666878\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 184, loss: 0.051422109827399254\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 185, loss: 0.05128058139234781\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 186, loss: 0.05114021245390177\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 187, loss: 0.051001012325286865\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 188, loss: 0.05086293909698725\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 189, loss: 0.05072600580751896\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 190, loss: 0.050590199418365955\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 191, loss: 0.050455489195883274\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 192, loss: 0.05032187420874834\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 193, loss: 0.05018933489918709\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 194, loss: 0.050057861022651196\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 195, loss: 0.04992745537310839\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 196, loss: 0.04979808162897825\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 197, loss: 0.04966974165290594\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 198, loss: 0.04954242613166571\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 199, loss: 0.04941611271351576\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 200, loss: 0.04929080419242382\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 201, loss: 0.049166472628712654\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 202, loss: 0.04904312454164028\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 203, loss: 0.048920731991529465\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 204, loss: 0.04879929404705763\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 205, loss: 0.04867880139499903\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 206, loss: 0.04855923820286989\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 207, loss: 0.048440598882734776\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 208, loss: 0.04832287319004536\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 209, loss: 0.048206047154963017\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 210, loss: 0.04809010960161686\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 211, loss: 0.04797505587339401\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 212, loss: 0.047860877588391304\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 213, loss: 0.04774755984544754\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 214, loss: 0.0476350924000144\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 215, loss: 0.047523465007543564\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 216, loss: 0.0474126823246479\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 217, loss: 0.04730272479355335\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 218, loss: 0.047193579375743866\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 219, loss: 0.04708524886518717\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 220, loss: 0.04697770718485117\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 221, loss: 0.04687096644192934\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 222, loss: 0.04676500428467989\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 223, loss: 0.046659816056489944\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 224, loss: 0.046555389650166035\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 225, loss: 0.04645173158496618\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 226, loss: 0.046348826959729195\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 227, loss: 0.04624664783477783\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 228, loss: 0.04614521563053131\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 229, loss: 0.04604450520128012\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 230, loss: 0.04594451282173395\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 231, loss: 0.04584523383527994\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 232, loss: 0.045746659860014915\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 233, loss: 0.045648783445358276\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 234, loss: 0.045551598072052\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 235, loss: 0.04545509070158005\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 236, loss: 0.045359269715845585\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 237, loss: 0.0452641025185585\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 238, loss: 0.04516960587352514\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 239, loss: 0.04507576394826174\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 240, loss: 0.04498256929218769\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 241, loss: 0.044890016317367554\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 242, loss: 0.044798108749091625\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 243, loss: 0.04470681492239237\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 244, loss: 0.04461614973843098\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 245, loss: 0.04452611040323973\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 246, loss: 0.04443667363375425\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 247, loss: 0.044347841292619705\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 248, loss: 0.04425961058586836\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 249, loss: 0.044171969406306744\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 250, loss: 0.04408490750938654\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 251, loss: 0.04399843979626894\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 252, loss: 0.043912542052567005\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 253, loss: 0.043827205896377563\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 254, loss: 0.043742443434894085\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 255, loss: 0.043658239766955376\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 256, loss: 0.04357459023594856\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 257, loss: 0.043491488322615623\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 258, loss: 0.04340891819447279\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 259, loss: 0.04332689568400383\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 260, loss: 0.043245404958724976\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 261, loss: 0.043164435774087906\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 262, loss: 0.04308399744331837\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 263, loss: 0.043004066683351994\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 264, loss: 0.04292465094476938\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 265, loss: 0.04284574184566736\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 266, loss: 0.04276733845472336\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 267, loss: 0.042689427733421326\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 268, loss: 0.042612007819116116\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 269, loss: 0.042535082437098026\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 270, loss: 0.04245864413678646\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 271, loss: 0.0423826826736331\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 272, loss: 0.04230719245970249\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 273, loss: 0.042232176288962364\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 274, loss: 0.04215762112289667\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 275, loss: 0.042083531618118286\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 276, loss: 0.04200989846140146\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 277, loss: 0.04193671327084303\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 278, loss: 0.04186398349702358\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 279, loss: 0.04179169982671738\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 280, loss: 0.041719856671988964\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 281, loss: 0.04164844285696745\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 282, loss: 0.0415774704888463\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 283, loss: 0.041506923735141754\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 284, loss: 0.041436802595853806\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 285, loss: 0.041367098689079285\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 286, loss: 0.04129782132804394\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 287, loss: 0.04122895281761885\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 288, loss: 0.04116049222648144\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 289, loss: 0.04109244793653488\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 290, loss: 0.04102479945868254\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 291, loss: 0.04095755238085985\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 292, loss: 0.04089069180190563\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 293, loss: 0.04082423448562622\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 294, loss: 0.040758166462183\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 295, loss: 0.040692479349672794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 296, loss: 0.04062717128545046\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 297, loss: 0.040562248788774014\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 298, loss: 0.040497696958482265\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 299, loss: 0.04043351951986551\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 300, loss: 0.04036971367895603\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 301, loss: 0.04030627105385065\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 302, loss: 0.040243194438517094\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 303, loss: 0.04018046800047159\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 304, loss: 0.04011810338124633\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 305, loss: 0.04005609406158328\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 306, loss: 0.039994440507143736\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 307, loss: 0.03993312222883105\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 308, loss: 0.039872155059129\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 309, loss: 0.03981153108179569\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 310, loss: 0.03975124331191182\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 311, loss: 0.03969129594042897\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 312, loss: 0.039631680119782686\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 313, loss: 0.03957238281145692\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 314, loss: 0.03951342590153217\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 315, loss: 0.03945479029789567\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 316, loss: 0.03939648391678929\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 317, loss: 0.03933848859742284\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 318, loss: 0.03928081085905433\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 319, loss: 0.039223446510732174\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 320, loss: 0.03916640114039183\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 321, loss: 0.03910966124385595\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 322, loss: 0.039053221233189106\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 323, loss: 0.038997095078229904\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 324, loss: 0.03894126741215587\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 325, loss: 0.038885737769305706\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 326, loss: 0.03883049916476011\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 327, loss: 0.038775558583438396\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 328, loss: 0.03872091369703412\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 329, loss: 0.038666554260998964\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 330, loss: 0.0386124849319458\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 331, loss: 0.03855870245024562\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 332, loss: 0.03850520076230168\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 333, loss: 0.03845197660848498\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 334, loss: 0.03839903557673097\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 335, loss: 0.03834636835381389\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 336, loss: 0.03829397959634662\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 337, loss: 0.03824185347184539\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 338, loss: 0.03819000208750367\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 339, loss: 0.038138415198773146\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 340, loss: 0.03808710025623441\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 341, loss: 0.03803604422137141\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 342, loss: 0.03798525780439377\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 343, loss: 0.037934715393930674\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 344, loss: 0.03788443887606263\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 345, loss: 0.03783441847190261\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 346, loss: 0.03778464952483773\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 347, loss: 0.03773513389751315\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 348, loss: 0.03768586507067084\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 349, loss: 0.03763684304431081\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 350, loss: 0.03758806874975562\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 351, loss: 0.03753953706473112\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 352, loss: 0.037491250317543745\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 353, loss: 0.037443204782903194\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 354, loss: 0.03739539487287402\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 355, loss: 0.0373478215187788\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 356, loss: 0.03730047307908535\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 357, loss: 0.03725337516516447\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 358, loss: 0.03720649844035506\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 359, loss: 0.037159846629947424\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 360, loss: 0.03711342951282859\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 361, loss: 0.03706724522635341\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 362, loss: 0.03702127141878009\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 363, loss: 0.036975529976189137\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 364, loss: 0.03693000553175807\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 365, loss: 0.0368847013451159\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 366, loss: 0.036839612759649754\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 367, loss: 0.03679474862292409\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 368, loss: 0.03675009310245514\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 369, loss: 0.036705652717500925\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 370, loss: 0.03666142327710986\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 371, loss: 0.036617396865040064\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 372, loss: 0.03657358651980758\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 373, loss: 0.03652997920289636\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 374, loss: 0.036486583296209574\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 375, loss: 0.03644339041784406\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 376, loss: 0.03640039311721921\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 377, loss: 0.0363576035015285\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 378, loss: 0.036315012723207474\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 379, loss: 0.03627262031659484\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 380, loss: 0.036230423022061586\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 381, loss: 0.0361884250305593\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 382, loss: 0.03614661889150739\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 383, loss: 0.03610500367358327\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 384, loss: 0.03606357052922249\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 385, loss: 0.036022343672811985\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 386, loss: 0.035981299821287394\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 387, loss: 0.03594044363126159\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 388, loss: 0.03589977417141199\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 389, loss: 0.035859286319464445\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 390, loss: 0.03581898985430598\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 391, loss: 0.03577886521816254\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 392, loss: 0.03573893057182431\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 393, loss: 0.03569917054846883\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 394, loss: 0.03565958747640252\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 395, loss: 0.0356201883405447\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 396, loss: 0.03558096243068576\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 397, loss: 0.03554190881550312\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 398, loss: 0.03550303215160966\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 399, loss: 0.03546432638540864\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 400, loss: 0.035425789188593626\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 401, loss: 0.03538742894306779\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 402, loss: 0.03534922935068607\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 403, loss: 0.03531120624393225\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 404, loss: 0.035273338202387094\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 405, loss: 0.03523564711213112\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 406, loss: 0.03519811388105154\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 407, loss: 0.03516074409708381\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 408, loss: 0.03512354660779238\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 409, loss: 0.035086494870483875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 410, loss: 0.03504961961880326\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 411, loss: 0.03501288779079914\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 412, loss: 0.03497632360085845\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 413, loss: 0.03493990935385227\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 414, loss: 0.03490366041660309\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 415, loss: 0.03486755723133683\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 416, loss: 0.034831615164875984\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 417, loss: 0.03479582304134965\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 418, loss: 0.03476017666980624\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 419, loss: 0.034724689088761806\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 420, loss: 0.03468935191631317\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 421, loss: 0.03465416422113776\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 422, loss: 0.03461912041530013\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 423, loss: 0.0345842270180583\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 424, loss: 0.03454947331920266\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 425, loss: 0.03451487002894282\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 426, loss: 0.034480408765375614\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 427, loss: 0.03444609511643648\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 428, loss: 0.03441191930323839\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 429, loss: 0.034377886448055506\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 430, loss: 0.0343439974822104\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 431, loss: 0.034310245886445045\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 432, loss: 0.034276632592082024\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 433, loss: 0.03424314968287945\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 434, loss: 0.03420981438830495\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 435, loss: 0.03417661041021347\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 436, loss: 0.03414354659616947\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 437, loss: 0.03411061270162463\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 438, loss: 0.03407781617715955\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 439, loss: 0.03404514491558075\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 440, loss: 0.034012613352388144\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 441, loss: 0.03398020705208182\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 442, loss: 0.03394793486222625\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 443, loss: 0.033915786538273096\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 444, loss: 0.033883772790431976\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 445, loss: 0.03385189129039645\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 446, loss: 0.033820131327956915\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 447, loss: 0.03378849755972624\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 448, loss: 0.03375699184834957\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 449, loss: 0.033725605346262455\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 450, loss: 0.03369434783235192\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 451, loss: 0.03366321325302124\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 452, loss: 0.03363220160827041\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 453, loss: 0.0336013101041317\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 454, loss: 0.03357054525986314\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 455, loss: 0.033539887983351946\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 456, loss: 0.033509367145597935\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 457, loss: 0.03347895201295614\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 458, loss: 0.033448657020926476\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 459, loss: 0.033418480306863785\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 460, loss: 0.03338842652738094\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 461, loss: 0.03335848497226834\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 462, loss: 0.03332865657284856\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 463, loss: 0.033298938535153866\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 464, loss: 0.03326933877542615\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 465, loss: 0.033239856362342834\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 466, loss: 0.03321048244833946\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 467, loss: 0.03318121982738376\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 468, loss: 0.033152075950056314\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 469, loss: 0.03312302986159921\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 470, loss: 0.03309411043301225\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 471, loss: 0.033065286464989185\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 472, loss: 0.03303657751530409\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 473, loss: 0.033007977064698935\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 474, loss: 0.03297947999089956\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 475, loss: 0.03295109001919627\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 476, loss: 0.032922806683927774\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 477, loss: 0.03289463324472308\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 478, loss: 0.03286656038835645\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 479, loss: 0.03283859696239233\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 480, loss: 0.032810729928314686\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 481, loss: 0.03278297185897827\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 482, loss: 0.03275531157851219\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 483, loss: 0.03272775514051318\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 484, loss: 0.03270029602572322\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 485, loss: 0.03267294820398092\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 486, loss: 0.03264569165185094\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 487, loss: 0.03261853335425258\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 488, loss: 0.03259148774668574\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 489, loss: 0.03256453201174736\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 490, loss: 0.032537673600018024\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 491, loss: 0.032510916236788034\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 492, loss: 0.032484252005815506\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 493, loss: 0.032457681372761726\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 494, loss: 0.03243121085688472\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 495, loss: 0.032404832541942596\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 496, loss: 0.032378556206822395\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 497, loss: 0.03235236741602421\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 498, loss: 0.03232627036049962\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 499, loss: 0.032300276681780815\n",
      "Test_acc: 1.0\n",
      "--------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "# 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线\n",
    "\n",
    "# 导入所需模块\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 导入数据，分别为输入特征和标签\n",
    "x_data = datasets.load_iris().data\n",
    "y_data = datasets.load_iris().target\n",
    "\n",
    "# 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）\n",
    "# seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致）\n",
    "np.random.seed(116)  # 使用相同的seed，保证输入特征和标签一一对应\n",
    "np.random.shuffle(x_data)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(y_data)\n",
    "tf.random.set_seed(116)\n",
    "\n",
    "# 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行\n",
    "x_train = x_data[:-30]\n",
    "y_train = y_data[:-30]\n",
    "x_test = x_data[-30:]\n",
    "y_test = y_data[-30:]\n",
    "\n",
    "# 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错\n",
    "x_train = tf.cast(x_train, tf.float32)\n",
    "x_test = tf.cast(x_test, tf.float32)\n",
    "\n",
    "# from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "# 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元\n",
    "# 用tf.Variable()标记参数可训练\n",
    "# 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）\n",
    "w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))\n",
    "b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))\n",
    "\n",
    "lr = 0.1  # 学习率为0.1\n",
    "train_loss_results = []  # 将每轮的loss记录在此列表中，为后续画loss曲线提供数据\n",
    "test_acc = []  # 将每轮的acc记录在此列表中，为后续画acc曲线提供数据\n",
    "epoch = 500  # 循环500轮\n",
    "loss_all = 0  # 每轮分4个step，loss_all记录四个step生成的4个loss的和\n",
    "\n",
    "# 训练部分\n",
    "for epoch in range(epoch):  #数据集级别的循环，每个epoch循环一次数据集\n",
    "     # enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标\n",
    "    for step, (x_train, y_train) in enumerate(train_db):  #batch级别的循环 ，每个step循环一个batch\n",
    "        with tf.GradientTape() as tape:  # with结构记录梯度信息\n",
    "            y = tf.matmul(x_train, w1) + b1  # 神经网络乘加运算\n",
    "            y = tf.nn.softmax(y)  # 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）\n",
    "            y_ = tf.one_hot(y_train, depth=3)  # 将标签值转换为独热码格式，方便计算loss和accuracy\n",
    "            loss = tf.reduce_mean(tf.square(y_ - y))  # 采用均方误差损失函数mse = mean(sum(y-out)^2)\n",
    "            loss_all += loss.numpy()  # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确\n",
    "        # 计算loss对各个参数的梯度\n",
    "        grads = tape.gradient(loss, [w1, b1])\n",
    "\n",
    "        # 实现梯度更新 w1 = w1 - lr * w1_grad    b = b - lr * b_grad\n",
    "        w1.assign_sub(lr * grads[0])  # 参数w1自更新\n",
    "        b1.assign_sub(lr * grads[1])  # 参数b自更新\n",
    "\n",
    "    # 每个epoch，打印loss信息\n",
    "    print(\"Epoch {}, loss: {}\".format(epoch, loss_all/4))\n",
    "    train_loss_results.append(loss_all / 4)  # 将4个step的loss求平均记录在此变量中\n",
    "    loss_all = 0  # loss_all归零，为记录下一个epoch的loss做准备\n",
    "\n",
    "    # 测试部分\n",
    "    # total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0\n",
    "    total_correct, total_number = 0, 0\n",
    "    for x_test, y_test in test_db:\n",
    "        # 使用更新后的参数进行预测\n",
    "        y = tf.matmul(x_test, w1) + b1\n",
    "        y = tf.nn.softmax(y)\n",
    "        pred = tf.argmax(y, axis=1)  # 返回y中最大值的索引，即预测的分类\n",
    "        # 将pred转换为y_test的数据类型\n",
    "        pred = tf.cast(pred, dtype=y_test.dtype)\n",
    "        # 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型\n",
    "        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)\n",
    "        # 将每个batch的correct数加起来\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        # 将所有batch中的correct数加起来\n",
    "        total_correct += int(correct)\n",
    "        # total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数\n",
    "        total_number += x_test.shape[0]\n",
    "    # 总的准确率等于total_correct/total_number\n",
    "    acc = total_correct / total_number\n",
    "    test_acc.append(acc)\n",
    "    print(\"Test_acc:\", acc)\n",
    "    print(\"--------------------------\")\n",
    "\n",
    "# 绘制 loss 曲线\n",
    "plt.title('Loss Function Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Loss')  # y轴变量名称\n",
    "plt.plot(train_loss_results, label=\"$Loss$\")  # 逐点画出trian_loss_results值并连线，连线图标是Loss\n",
    "plt.legend()  # 画出曲线图标\n",
    "plt.show()  # 画出图像\n",
    "\n",
    "# 绘制 Accuracy 曲线\n",
    "plt.title('Acc Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Acc')  # y轴变量名称\n",
    "plt.plot(test_acc, label=\"$Accuracy$\")  # 逐点画出test_acc值并连线，连线图标是Accuracy\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
