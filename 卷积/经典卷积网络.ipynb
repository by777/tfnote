{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 经典卷积网络\n",
    "\n",
    "\n",
    "*在统计卷积神经网络层数时，一般只统计卷积计算层和全连接计算层*\n",
    "\n",
    "\n",
    ">CBAPD\n",
    "\n",
    "##### LeNet(1998)\n",
    "\n",
    "通过共享卷积核减少了网络参数\n",
    "\n",
    "##### AlexNet(2012) \n",
    "\n",
    "诞生于2012，当年ImageNet竞赛的冠军。使用relu激活函数提升了计算速度，Dropout缓解了过拟合\n",
    "\n",
    "##### VGGNet(2014) \n",
    "\n",
    "当年ImageNet竞赛的亚军，使用小尺寸卷积核，在减少参数的同时，提高了识别准确率，结构规整适合网络加速\n",
    "\n",
    "##### InceptionNet(2014) \n",
    "\n",
    "当年ImageNet冠军，InceptionNet引入了`Inception结构块`，无论是GoogLeNet，也就是Inception v1，还是InceptionNet的后续版本v2、v3、v4，都是基于此。\n",
    "\n",
    "+ 在同一层网络内使用不同尺寸的卷积核，提升了模型感知力。\n",
    "+ 使用了批标准化，缓解了梯度消失。\n",
    "+ 通过在同一层网络内使用不同尺寸的卷积核，提取不同尺寸的特征。\n",
    "+ 通过 1 * 1的卷积核作用到输入特征图的每个像素点。\n",
    "+ 通过设定少于输入特征图深度的1 * 1卷积核个数，减少了输出特征图的深度，起到了降维的作用。减少了参数量和计算量。\n",
    "\n",
    "##### ResNet(2015)\n",
    "\n",
    "当年ImageNet冠军，ResNet提出了层间残差跳连，引入了前方信息，缓解梯度消失，是神经网络层数增加成为可能\n",
    "\n",
    "+ Inception块中的 “+”是沿着深度方向叠加\n",
    "+ ResNet块中的“+”是特征图对应元素值相加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet\n",
    "\n",
    "    class LeNet5(Model):\n",
    "        def __init__(self):\n",
    "            super(LeNet,self).__init__()\n",
    "            # 1\n",
    "            self.c1 = Conv2D(filters=6,kernel_size=(5,5),activation=\"sigmodi)\n",
    "            self.p1 = MaxPool2D(pool_size=(2,2),strides=2)\n",
    "            # 2\n",
    "            self.c2 = Conv2D(filters=16,kernel_size=(5,5),activation=\"sigmoid\")\n",
    "            self.p2 = MaxPool2D(pool_size=(2,2),strides=2)\n",
    "            \n",
    "            self.flatten = Flatten()\n",
    "            # 3\n",
    "            self.f1 = Dense(120,activation=\"sigmoid)\n",
    "            # 4\n",
    "            self.f2 = Dense(84,activation=\"sigmoid\")\n",
    "            # 5\n",
    "            self.f3 = Dense(10,activation=\"softmax\")\n",
    "        def call(self):\n",
    "            ....\n",
    "            return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet\n",
    "\n",
    "    class AlexNet8(Model):\n",
    "        def __init__(self):\n",
    "            super(AlexNet,self).__init__()\n",
    "            # 1\n",
    "            self.c1 = Conv2D(filters=96,kernel_size=(3,3))\n",
    "            self.b1 = BatchNormalization()\n",
    "            self.a1 = Activation('relu')\n",
    "            self.p1 = MaxPool2D(pool_size=(3,3),strides=2)\n",
    "            # 2\n",
    "            self.c2 = Conv2D(filters=256,kernel_size=(3,3))\n",
    "            self.b2 = BatchNormalization()\n",
    "            self.a2 = Activation('relu')\n",
    "            self.p2 = MaxPool2D(pool_size=(3,3),strides=2)\n",
    "            # 3\n",
    "            self.c3 = Conv2D(filters=284,kernel_size=(3,3),\n",
    "                        padding='same',activation=\"relu\")\n",
    "            # 4\n",
    "            self.c4 = Conv2D(filters=384,kernel_size=(3,3),\n",
    "                        padding='same',activation=\"relu\")\n",
    "            # 5\n",
    "            self.c5 = Conv2D(filters=256,kernel_size=(3,3),\n",
    "                         padding='same',activation=\"relu\")\n",
    "            self.p3 = MaxPool2D(pool_size=(3,3),strides=2)\n",
    "            \n",
    "            self.flatten = Flatten()\n",
    "            # 6\n",
    "            self.f1 = Dense(2048,activation=\"relu\")\n",
    "            self.d1 = Dropout(0.5)\n",
    "            # 7\n",
    "            self.f2 = Dense(2048,activation=\"relu\")\n",
    "            self.d2 = Dropout(0.5)\n",
    "            # 8\n",
    "            self.f3 = Dense(10,activation=\"softmax\")\n",
    "            \n",
    "        def call(self,x):\n",
    "            x = self.c1(x)\n",
    "            ....\n",
    "            return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG\n",
    "\n",
    "    class VGG16(Model):\n",
    "        def __init__(self):\n",
    "        super(VGG16,self).__init__()\n",
    "        # 1\n",
    "        self.c1 = Conv2D(filters=64,kernel_size=(3,3),padding=\"same\")\n",
    "        self.b1 = BatchNormalization()\n",
    "        self.a1 = Activation(\"relu\")\n",
    "        # 2\n",
    "        self.c2 = Conv2D(filters=64,kernel_size=(3,3),padding=\"same\")\n",
    "        self.b2 = BatchNormalization()\n",
    "        self.a2 = Activation(\"relu\")\n",
    "        self.p1 = MaxPool2D(pool_size=(2,2),strides=2,padding=\"same\")\n",
    "        self.dropout(0.2)\n",
    "        # 3\n",
    "        self.c3 = Conv2D(filters=128,kernel_size=(3,3),padding=\"same\")\n",
    "        self.b3 = BatchNormalization()\n",
    "        self.a3 = Activation(\"relu\")\n",
    "        # 4\n",
    "        self.c4 = Conv2D(filters=128,kernel_size=(3,3),padding=\"same\")\n",
    "        self.b4 = BatchNormalization()\n",
    "        self.a4 = Activation('relu') \n",
    "        self.p2 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "        self.d2 = Dropout(0.2)\n",
    "        # 5\n",
    "        self.c5 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')\n",
    "        self.b5 = BatchNormalization()  \n",
    "        self.a5 = Activation('relu') \n",
    "        # 6\n",
    "        self.c6 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')\n",
    "        self.b6 = BatchNormalization() \n",
    "        self.a6 = Activation('relu')  \n",
    "        # 7\n",
    "        self.c7 = Conv2D(filters=256, kernel_size=(3, 3), padding='same')\n",
    "        self.b7 = BatchNormalization()\n",
    "        self.a7 = Activation('relu')\n",
    "        self.p3 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "        self.d3 = Dropout(0.2)\n",
    "        # 8\n",
    "        self.c8 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b8 = BatchNormalization() \n",
    "        self.a8 = Activation('relu')  \n",
    "        # 9\n",
    "        self.c9 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b9 = BatchNormalization()  \n",
    "        self.a9 = Activation('relu')  \n",
    "        # 10\n",
    "        self.c10 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b10 = BatchNormalization()\n",
    "        self.a10 = Activation('relu')\n",
    "        self.p4 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "        self.d4 = Dropout(0.2)\n",
    "        # 11\n",
    "        self.c11 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b11 = BatchNormalization() \n",
    "        self.a11 = Activation('relu')  \n",
    "        # 12\n",
    "        self.c12 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b12 = BatchNormalization() \n",
    "        self.a12 = Activation('relu')  \n",
    "        # 13\n",
    "        self.c13 = Conv2D(filters=512, kernel_size=(3, 3), padding='same')\n",
    "        self.b13 = BatchNormalization()\n",
    "        self.a13 = Activation('relu')\n",
    "        self.p5 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "        self.d5 = Dropout(0.2)\n",
    "        # 14\n",
    "        self.flatten = Flatten()\n",
    "        self.f1 = Dense(512, activation='relu')\n",
    "        self.d6 = Dropout(0.2)\n",
    "        # 15\n",
    "        self.f2 = Dense(512, activation='relu')\n",
    "        self.d7 = Dropout(0.2)\n",
    "        # 16\n",
    "        self.f3 = Dense(10, activation='softmax')\n",
    "    def call(self,x):\n",
    "        ...\n",
    "        return y\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionNet\n",
    "    # Inception结构块（CBN）\n",
    "    class ConvBNRelu(Model):\n",
    "        def __init__(self)__:\n",
    "        super(ConvBNRelu,self).__init__()\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            Conv2D(ch,kernelsz=3,strides=1,padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Activation('relu')\n",
    "        ])\n",
    "        def call(self,x):\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "            \n",
    "    class InceptionBlk(Model):\n",
    "        def __init__(self):\n",
    "            super(InceptionBlk,self).__init__()\n",
    "            # 第一个分支\n",
    "            self.c1 = ConvBNRelu(ch,kernelsz=1,strides=strides)\n",
    "            # 第二个分支\n",
    "            self.c2_1 = ConvBNRelu(ch,kernelsz=1,strides=strides)\n",
    "            self.c2_2 = ConvBNRelu(ch,kernelsz=3,strides=1)\n",
    "            # 第三个分支\n",
    "            self.c3_1 = ConvBNRelu(ch,kernelsz=1,strides=strides)\n",
    "            self.c3_2 = ConvBNRelu(ch,kernelsz=5,strides=1)\n",
    "            # 第四个分支\n",
    "            self.p4_1 = MaxPool2D(3,strides=1,padding='same')\n",
    "            self.c4_2 = ConvBNRelu(ch,kernelsz=1,strides=strides)\n",
    "        def call(self,x):\n",
    "            ...\n",
    "            # 【四个分支的输出，堆叠在一起，axis=3指定堆叠的维度沿着深度方向】\n",
    "            x = tf.concat([,,,],axis=3)\n",
    "            return x\n",
    "            \n",
    "    class Inception10(Model):\n",
    "        # 默认输出深度是16\n",
    "        def __init__(self,num_blocks,num_classes,init_ch=16,**kwargs):#init_ch是卷积核数目\n",
    "            super(Inception10,self).__init__(**kwargs)\n",
    "            self.in_channels = init_ch\n",
    "            self.out_channels = init_ch\n",
    "            self.num_blocks = num_blocks\n",
    "            self.init_ch = init_ch\n",
    "            \n",
    "            self.c1 = ConvBNRelu(init_ch)\n",
    "            # 随后是4个Inception结构块顺序相连\n",
    "            # 每两个Inception结构块构成一个block\n",
    "            # block中的第一个Inception，卷积步长2，第二个1\n",
    "            self.blocks = tf.keras.models.Sequential()\n",
    "            for block_id in range(num_blocks)：\n",
    "                for layer_id in range(2):\n",
    "                    if layer_id == 0:\n",
    "                        # strides=2使得输出特征图尺寸减半，因此把输出特征图深度加深\n",
    "                        block = InceptionBlk(self.out_channels,strides=2)\n",
    "                    else:\n",
    "                        block = InceptionBlk(self.out_channels,strides=1)\n",
    "                    self.block.add(block)\n",
    "                # 以尽可能保持特征抽取中信息的承载量保持一致\n",
    "                self.out_channels *= 2\n",
    "            self.p1 = GlobalAveragePooling2D()\n",
    "            self.f1 = Dense(num_classes,activation=\"softmax\")\n",
    "        def call(self,x):\n",
    "            x = self.c1(x)\n",
    "            x = self.blocks(x)\n",
    "            x = self.p1(x)\n",
    "            y = self.f1(x)\n",
    "        return y\n",
    "    model = Inception(num_blocks=2,num_classes=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet\n",
    "\n",
    "    class ResnetBlock(Model):\n",
    "        def __init__(self,filters,strides=1,residual_path=False):\n",
    "            self.filters = filters\n",
    "            self.strides = strides\n",
    "            self.residual_path = residual_path\n",
    "            \n",
    "            self.c1 = Conv2D(filters,(3,3),strides=strides,padding='same',use_bias=False)\n",
    "            self.b1 = BatchNormalization()\n",
    "            self.a1 = Activation(\"relu\")\n",
    "            \n",
    "            self.c2 = Conv2D(filters,(3,3),strides=1,padding=\"same\",use_bias=False)\n",
    "            self.b2 = BatchNormalization()\n",
    "            \n",
    "            # 当residual_path为True时，对输入进行下采样，即用1*1的卷积核做卷积操作，\n",
    "            # 保证x能和F(x)维度相同\n",
    "            if residual_path :\n",
    "                self.down_c1 = Conv2D(filters,(1,1),strides=strides,padding='same',use_bias=False)\n",
    "                self.down)b1 = BatchNormalization()\n",
    "            self.a2 = Activation('relu')\n",
    "        def call(self,inputs):\n",
    "            residual = inputs\n",
    "            x = self.c1(inputs)\n",
    "            x = self.b1(x)\n",
    "            x = self.a1(x)\n",
    "            x = self.c2(x)\n",
    "            y = self.b2(x)\n",
    "            if self.residual_path:\n",
    "                residual = self.down_c1(inputs)\n",
    "                residual = self.down_b1(residual)\n",
    "            # 最后的输出是两部分的和，即F(x) + x或F(x) + Wx，再过激活\n",
    "            out = self.a2(y + residual)\n",
    "            return out\n",
    "            \n",
    "    class ResNet18(Model):\n",
    "        # block_list表示每个block有几个卷积层\n",
    "        def __init__(self,block_list,initial_filters=64):\n",
    "            super(ResNet18,self).__init__()\n",
    "            # 共有几个block\n",
    "            self.num_list = len(block_list)\n",
    "            self.block_list = block_list\n",
    "            self.out_filters = initial_filters\n",
    "            self.c1 = Conv2D(self.out_filters,(3,3),strides=1,padding=\"same\",use_bias=False)\n",
    "            self.b1 = BatchNormalization()\n",
    "            self.a1 = Activation(\"relu\")\n",
    "            self.blocks = tf.keras.Sequential()\n",
    "            # 第几个ResNet Block\n",
    "            for block_id in range(len(block_list)):\n",
    "                # 第几个卷积层\n",
    "                for lay_id in range(block_list[block_id]):\n",
    "                    # 对除第一个block以外的每个block的输入进行下采样\n",
    "                    if block_id != 0 and layer_id == 0:\n",
    "                        # 虚线连接\n",
    "                        block = ResnetBlock(self.out_filters, strides=2, residual_path=True)\n",
    "                    else:\n",
    "                        # 实线连接\n",
    "                        block = ResnetBlock(self.out_filters, residual_path=False)\n",
    "                    self.blocks.add(block)  # 将构建好的block加入resnet\n",
    "                self.out_filters *= 2  # 下一个block的卷积核数是上一个block的2倍\n",
    "            self.p1 = tf.keras.layers.GlobalAveragePooling2D()\n",
    "            self.f1 = tf.keras.layers.Dense(10, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2())\n",
    "        def call(self, inputs):\n",
    "            x = self.c1(inputs)\n",
    "            x = self.b1(x)\n",
    "            x = self.a1(x)\n",
    "            x = self.blocks(x)\n",
    "            x = self.p1(x)\n",
    "            y = self.f1(x)\n",
    "            return y\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
